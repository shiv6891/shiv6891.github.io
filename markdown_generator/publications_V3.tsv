pub_date	title	venue	excerpt	citation	url_slug	paper_url
2024-03-20	Emotion-Aware Multimodal Fusion for Meme Emotion Detection	IEEE Transactions on Affective Computing	Memes are widely used on social media to express opinions, but current methods struggle to capture their emotional dimensions, relying on large datasets and lacking generalization. We introduce MOOD (Meme emOtiOns Dataset) with six emotions and ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a neural framework that effectively models visual-emotional cues and cross-modal fusion. ALFRED outperforms existing methods by 4.94% F1, excels in the Memotion task, and generalizes well on HarMeme and Dank Memes datasets. It also offers interpretability through attention maps. We address the challenges of analyzing memes due to complex modality-specific cues.	S. Sharma, R. S, M. S. Akhtar and T. Chakraborty, "Emotion-Aware Multimodal Fusion for Meme Emotion Detection," in IEEE Transactions on Affective Computing, doi: 10.1109/TAFFC.2024.3378698.	tac24-emotion-memes	https://ieeexplore.ieee.org/document/10475492
2024-05-16	MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing	ACLâ€™24 (Findings)	Memes, widely used for humor and propaganda, need exploration for potential harm. Previous studies have focused on detecting harm and providing explanations in closed settings. We introduce MemeMQA, a multimodal question-answering framework designed to provide accurate responses and coherent explanations for structured questions about memes. We present MemeMQACorpus, a dataset with 1,880 questions related to 1,122 memes, including answer-explanation pairs. Our proposed ARSENAL framework, leveraging LLMs, outperforms baselines by ~18% in answer prediction accuracy and shows superior text generation in lexical and semantic alignment. We evaluate ARSENAL's robustness through diverse question sets and modality-specific assessments, enhancing our understanding of meme interpretation in multimodal communication.	Siddhant Agarwal, Shivam Sharma, Preslav Nakov, and Tanmoy Chakraborty. 2024. MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing. In <i>Findings of the Association for Computational Linguistics: ACL 2024</I>, Bangkok, Thailand. Association for Computational Linguistics.	acl23-mememqa-memes	https://arxiv.org/abs/2405.11215
2024-07-10	Factuality Challenges in the Era of Large Language Models and Opportunities for Fact-Checking	Nature Machine Intelligence	The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative AI.	Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty*, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni, Factuality Challenges in the Era of Large Language Models, In <i>Nature Machine Intelligence/i>, 2024.	nmi-factuality-perspective	https://arxiv.org/abs/2310.05189